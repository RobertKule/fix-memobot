# backend/app/llm_service.py

import os
import json
import re
import csv
from typing import List, Dict, Any, Optional

from dotenv import load_dotenv
from datetime import datetime

load_dotenv()

# ======================
# CONFIG G√âN√âRALE
# ======================

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemma-3-1b-it")

# Cache global des sujets du CSV
SUJETS_CSV_CACHE: List[Dict[str, Any]] = []
SUJETS_CSV_INITIALIZED: bool = False

# =============================
# CONFIGURATION LANGCHAIN
# =============================

llm = None
json_parser = None

try:
    from langchain_google_genai import (
        ChatGoogleGenerativeAI,
        GoogleGenerativeAIEmbeddings,
    )
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
    from langchain_core.exceptions import OutputParserException
    from langchain_community.vectorstores import Chroma
    from langchain_core.documents import Document

    if GOOGLE_API_KEY:
        llm = ChatGoogleGenerativeAI(
            model=GEMINI_MODEL,
            google_api_key=GOOGLE_API_KEY,
            temperature=0.2,
            max_output_tokens=2048,
        )
        json_parser = JsonOutputParser()
        print("‚úÖ LangChain avec Gemini configur√©")
    else:
        print("‚ö†Ô∏è GOOGLE_API_KEY non configur√©e")
        llm = None
        json_parser = None

except ImportError as e:
    print(f"‚ùå LangChain non disponible: {e}")
    llm = None
    json_parser = None
    Chroma = None
    Document = None
    GoogleGenerativeAIEmbeddings = None

# ======================
# CHARGEMENT CSV SUJETS
# ======================

def load_sujets_csv(path: Optional[str] = None) -> List[Dict[str, Any]]:
    """
    Charge la base de sujets √©tudiants depuis le CSV pour servir de contexte √† l'IA.
    """
    global SUJETS_CSV_CACHE, SUJETS_CSV_INITIALIZED

    if SUJETS_CSV_CACHE:
        SUJETS_CSV_INITIALIZED = True
        return SUJETS_CSV_CACHE

    if path is None:
        path = os.path.join(
            os.path.dirname(__file__),
            "..",
            "data",
            "Sujet_EtudiantsB.csv",
        )

    sujets: List[Dict[str, Any]] = []
    try:
        with open(path, mode="r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f, delimiter=";")
            for row in reader:
                sujets.append(
                    {
                        "titre": row.get("titre") or row.get("Titre") or "",
                        "domaine": row.get("domaine") or row.get("Domaine") or "",
                        "facult√©": row.get("faculte") or row.get("Facult√©") or "",
                        "niveau": row.get("niveau") or row.get("Niveau") or "",
                        "probl√©matique": row.get("problematique") or row.get("Probl√©matique") or "",
                        "description": row.get("description") or row.get("Description") or "",
                        "keywords": row.get("keywords") or row.get("MotsCles") or "",
                        "statut": row.get("statut") or row.get("Statut") or "",
                    }
                )
        SUJETS_CSV_CACHE = sujets
        SUJETS_CSV_INITIALIZED = True
        print(f"‚úÖ Charg√© {len(sujets)} sujets depuis Sujet_EtudiantsB.csv")
    except Exception as e:
        print(f"‚ö†Ô∏è Impossible de charger Sujet_EtudiantsB.csv: {e}")
        SUJETS_CSV_CACHE = []
        SUJETS_CSV_INITIALIZED = False

    return SUJETS_CSV_CACHE

def get_llm_status() -> Dict[str, Any]:
    return {
        "llm_available": llm is not None,
        "sujets_csv_initialized": SUJETS_CSV_INITIALIZED,
        "sujets_csv_count": len(SUJETS_CSV_CACHE),
    }

# ======================
# VECTEUR STORE SUJETS CSV + CRIT√àRES DOYEN
# ======================

SUJETS_VECTORSTORE = None  # objet Chroma

def get_acceptance_criteria() -> Dict[str, Any]:
    """
    Retourne les crit√®res d'acceptation / rejet des sujets de m√©moire,
    bas√©s explicitement sur les directives du doyen.
    """
    return {
        "crit√®res_acceptation": [
            "Capacit√© de l‚Äô√©tudiant √† traiter le sujet en tenant compte de la disponibilit√© des donn√©es",
            "Complexit√© du sujet adapt√©e au niveau de l‚Äô√©tudiant",
            "Sujet r√©alisable dans les contraintes temporelles et financi√®res",
            "Pertinence du sujet par rapport au domaine de sp√©cialisation",
            "Caract√®re innovant ou apport original du travail propos√©",
            "Le sujet d√©passe un simple travail pratique de cours (approche recherche, analyse, r√©flexion)",
            "Probl√©matique clairement formul√©e, pr√©cise et pertinente",
            "Approche m√©thodologique coh√©rente, solide et adapt√©e aux objectifs",
        ],
        "crit√®res_rejet": [
            "Formulation grammaticale du sujet incorrecte ou peu compr√©hensible",
            "Sujet inad√©quat avec le niveau d‚Äôun travail de fin d‚Äô√©tudes",
            "Sujet inad√©quat avec la sp√©cialit√© concern√©e",
            "Sujet d√©j√† trait√© de mani√®re plus pertinente ou plus approfondie sans valeur ajout√©e claire",
        ],
        "conseils_pratiques": [
            "V√©rifiez que le sujet est r√©alisable avec les donn√©es et les ressources dont vous disposez.",
            "Adaptez la complexit√© du sujet √† votre niveau (Licence, Master, etc.).",
            "Expliquez en quoi votre travail est diff√©rent et plus riche qu‚Äôun simple projet de cours.",
            "Soignez particuli√®rement la formulation du titre et de la probl√©matique (clart√©, fran√ßais correct).",
            "Clarifiez votre approche m√©thodologique : quelles √©tapes, quelles donn√©es, quelles m√©thodes ?",
        ],
        "message_doyen": (
            "En g√©n√©ral, un m√©moire est jug√© acceptable lorsqu‚Äôil respecte plusieurs exigences, "
            "notamment: (1) la capacit√© de l‚Äô√©tudiant √† traiter le sujet, en tenant compte de la "
            "disponibilit√© des donn√©es, de la complexit√© des concepts au regard du niveau de l‚Äô√©tudiant, "
            "ainsi que des contraintes temporelles et financi√®res; (2) la pertinence du sujet par rapport "
            "au domaine de sp√©cialisation; (3) le caract√®re innovant du travail propos√©; (4) la distinction "
            "du sujet par rapport √† un simple travail pratique de cours; (5) la clart√© et la pertinence de la "
            "probl√©matique √† traiter; (6) la coh√©rence et la solidit√© des approches m√©thodologiques retenues. "
            "Les motifs fr√©quents de rejet concernent: (1) une mauvaise formulation grammaticale du sujet; "
            "(2) une inad√©quation avec le niveau d‚Äôun travail de fin d‚Äô√©tudes ou avec la sp√©cialit√©; (3) "
            "le fait que le sujet ait d√©j√† √©t√© trait√© de mani√®re plus pertinente ou approfondie."
        ),
    }

def build_sujets_vectorstore(persist_directory: Optional[str] = None):
    """
    Construit (ou recharge) un vecteur store (Chroma) √† partir:
    - de la base CSV Sujet_EtudiantsB.csv
    - des crit√®res du doyen

    Si persist_directory est fourni et existe d√©j√†, on recharge au lieu de reconstruire.
    """
    global SUJETS_VECTORSTORE

    if SUJETS_VECTORSTORE is not None:
        return SUJETS_VECTORSTORE

    if not llm or not Chroma or not GoogleGenerativeAIEmbeddings or not Document:
        print("‚ö†Ô∏è LLM/embeddings non dispo, pas de vecteur store.")
        return None

    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

        # 1) Si on a un dossier de persistance existant, on recharge
        if persist_directory and os.path.isdir(persist_directory) and os.listdir(persist_directory):
            try:
                SUJETS_VECTORSTORE = Chroma(
                    embedding_function=embeddings,
                    persist_directory=persist_directory,
                )
                print(f"‚úÖ Vector store recharg√© depuis {persist_directory}")
                return SUJETS_VECTORSTORE
            except Exception as e:
                print(f"‚ö†Ô∏è Impossible de recharger le vecteur store existant, reconstruction: {e}")

        # 2) Sinon, on reconstruit √† partir du CSV + crit√®res
        sujets = load_sujets_csv()
        docs: List[Document] = []

        for i, s in enumerate(sujets):
            content = (
                f"Titre: {s.get('titre','')}\n"
                f"Domaine: {s.get('domaine','')}\n"
                f"Niveau: {s.get('niveau','')}\n"
                f"Facult√©: {s.get('facult√©','')}\n"
                f"Probl√©matique: {s.get('probl√©matique','')}\n"
                f"Description: {s.get('description','')}\n"
                f"Mots-cl√©s: {s.get('keywords','')}\n"
                f"Statut: {s.get('statut','')}\n"
            )
            docs.append(
                Document(
                    page_content=content,
                    metadata={
                        "source": "csv_sujet",
                        "index": i,
                        "titre": s.get("titre", ""),
                        "domaine": s.get("domaine", ""),
                        "niveau": s.get("niveau", ""),
                        "statut": s.get("statut", ""),
                    },
                )
            )

        # Ajouter un document avec les crit√®res du doyen
        criteria = get_acceptance_criteria()
        criteres_txt = (
            "CRIT√àRES D'ACCEPTATION:\n- "
            + "\n- ".join(criteria["crit√®res_acceptation"])
            + "\n\nCRIT√àRES DE REJET:\n- "
            + "\n- ".join(criteria["crit√®res_rejet"])
            + "\n\nMESSAGE DU DOYEN:\n"
            + criteria.get("message_doyen", "")
        )

        docs.append(
            Document(
                page_content=criteres_txt,
                metadata={"source": "doyen_criteria"},
            )
        )

        embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

        if persist_directory:
            os.makedirs(persist_directory, exist_ok=True)
            SUJETS_VECTORSTORE = Chroma.from_documents(
                documents=docs,
                embedding=embeddings,
                persist_directory=persist_directory,
            )
        else:
            SUJETS_VECTORSTORE = Chroma.from_documents(
                documents=docs,
                embedding=embeddings,
            )

        print(f"‚úÖ Vector store construit avec {len(docs)} documents")
        return SUJETS_VECTORSTORE

    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la construction du vecteur store: {e}")
        SUJETS_VECTORSTORE = None
        return None

def search_sujets_context(query: str, k: int = 5) -> List[Document]:
    """
    Recherche les documents les plus proches d'une requ√™te.
    Utilis√© pour fournir du contexte √† l'IA (exemples r√©els, crit√®res, etc.)
    """
    vs = build_sujets_vectorstore()
    if not vs:
        return []
    try:
        return vs.similarity_search(query, k=k)
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la recherche dans le vecteur store: {e}")
        return []

# ======================
# ANALYSE DE SUJET
# ======================

def get_fallback_analysis(sujet_data: Dict[str, Any]) -> Dict[str, Any]:
    """Analyse de secours sans IA"""
    return {
        "pertinence": 75,
        "points_forts": [
            f"Sujet dans le domaine: {sujet_data.get('domaine', 'G√©n√©ral')}",
            "Probl√©matique identifi√©e dans les donn√©es",
            f"Niveau adapt√©: {sujet_data.get('niveau', 'L3')}",
        ],
        "points_faibles": [
            "Analyse automatique limit√©e sans IA",
            "Suggestions g√©n√©riques",
            "Validation humaine requise",
        ],
        "suggestions": [
            "Consulter un enseignant r√©f√©rent pour validation",
            "Pr√©ciser la m√©thodologie de recherche",
            "D√©finir des objectifs sp√©cifiques et mesurables",
        ],
        "recommandations": [
            "Sujet potentiellement int√©ressant √† approfondir",
            "Valider la faisabilit√© avec un expert",
            "√âtudier des travaux similaires pour inspiration",
        ],
    }

def analyser_sujet(sujet_data: Dict[str, Any]) -> Dict[str, Any]:
    """Analyse un sujet avec LangChain, en tenant compte des crit√®res du doyen et de la base CSV."""
    if not llm:
        return get_fallback_analysis(sujet_data)

    criteria = get_acceptance_criteria()

    # Recherche de contexte pertinent (sujets similaires + crit√®res du doyen)
    query = (
        f"{sujet_data.get('titre','')} "
        f"{sujet_data.get('domaine','')} "
        f"{sujet_data.get('niveau','')} "
        f"{sujet_data.get('keywords','')}"
    )
    retrieved_docs = search_sujets_context(query, k=5)

    init_note = (
        "NOTE: La base r√©elle de sujets √©tudiants n'est pas encore enti√®rement initialis√©e, "
        "l'analyse repose donc surtout sur les crit√®res du doyen et quelques exemples partiels.\n"
        if not SUJETS_CSV_INITIALIZED
        else ""
    )

    prompt_template = """
    Tu es un expert en √©valuation de sujets de m√©moire universitaire (MemoBot).
    Tu dois √©valuer un sujet comme le ferait un doyen d'universit√©.

    === NOTE D'√âTAT ===
    {init_note}

    === DIRECTIVES DU DOYEN (CRIT√àRES) ===
    Crit√®res d'acceptation principaux:
    {criteres_acceptation}

    Crit√®res de rejet fr√©quents:
    {criteres_rejet}

    Message du doyen:
    {message_doyen}

    === CONTEXTE R√âEL (BASE SUJETS + DOYEN) ===
    Voici quelques extraits pertinents issus de notre base interne (sujets r√©els + crit√®res du doyen):
    {contexte_retrieved}

    === SUJET √Ä ANALYSER ===
    TITRE: {titre}
    DOMAINE: {domaine}
    NIVEAU: {niveau}
    FACULT√â: {facult√©}
    PROBL√âMATIQUE: {problematique}
    DESCRIPTION: {description}
    MOTS-CL√âS: {keywords}

    === T√ÇCHE ===
    Analyse ce sujet de m√©moire en respectant les crit√®res du doyen et en te basant sur les exemples.
    Fais une analyse d√©taill√©e selon ces crit√®res:
    1. Pertinence g√©n√©rale (0-100%)
    2. Points forts (3-5 points)
    3. Points faibles (2-3 points)
    4. Suggestions d'am√©lioration (3-5 suggestions)
    5. Recommandations finales (2-3 recommandations)

    R√©ponds en JSON avec cette structure EXACTE:
    {{
        "pertinence": 85,
        "points_forts": ["point1", "point2", "point3"],
        "points_faibles": ["point1", "point2"],
        "suggestions": ["suggestion1", "suggestion2", "suggestion3"],
        "recommandations": ["recommandation1", "recommandation2"]
    }}
    """

    try:
        # Concat√©nation du contenu des documents r√©cup√©r√©s
        contexte_retrieved = ""
        for d in retrieved_docs:
            contexte_retrieved += f"\n---\n{d.page_content}\n"

        prompt = ChatPromptTemplate.from_template(prompt_template)
        chain = prompt | llm | StrOutputParser()

        raw = chain.invoke(
            {
                "titre": sujet_data.get("titre", ""),
                "domaine": sujet_data.get("domaine", ""),
                "niveau": sujet_data.get("niveau", ""),
                "facult√©": sujet_data.get("facult√©", ""),
                "problematique": sujet_data.get("probl√©matique", sujet_data.get("problematique", "")),
                "description": sujet_data.get("description", ""),
                "keywords": sujet_data.get("keywords", ""),
                "criteres_acceptation": "\n- " + "\n- ".join(criteria["crit√®res_acceptation"]),
                "criteres_rejet": "\n- " + "\n- ".join(criteria["crit√®res_rejet"]),
                "message_doyen": criteria.get("message_doyen", ""),
                "contexte_retrieved": contexte_retrieved or "Pas de contexte disponible.",
                "init_note": init_note,
            }
        )

        # Nettoyage de la sortie (enlever ```json, ``` etc.)
        cleaned = raw.strip()
        cleaned = cleaned.replace("```json", "").replace("```", "").strip()

        try:
            parsed = json.loads(cleaned)
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è Erreur JSON brute dans analyser_sujet: {e}")
            print(cleaned)
            return get_fallback_analysis(sujet_data)

        if not isinstance(parsed, dict):
            return get_fallback_analysis(sujet_data)

        for key in ["pertinence", "points_forts", "points_faibles", "suggestions", "recommandations"]:
            if key not in parsed:
                return get_fallback_analysis(sujet_data)

        return parsed

    except Exception as e:
        print(f"‚ö†Ô∏è Erreur dans analyser_sujet: {e}")
        return get_fallback_analysis(sujet_data)

# ======================
# RECOMMANDATION DE SUJETS
# ======================

def fallback_recommendation(interests: List[str], sujets: List[Dict]) -> List[Dict[str, Any]]:
    """Recommandation de secours sans IA"""
    results = []

    if not sujets:
        return results

    for sujet in sujets[:5]:
        score = 0
        matching_points = []

        titre = sujet.get("titre", "").lower()
        keywords = sujet.get("keywords", "").lower()
        domaine = sujet.get("domaine", "").lower()

        for interest in interests:
            interest_lower = interest.lower()

            if interest_lower in titre:
                score += 30
                matching_points.append(f"Int√©r√™t '{interest}' dans le titre")

            if interest_lower in keywords:
                score += 25
                matching_points.append(f"Int√©r√™t '{interest}' dans les mots-cl√©s")

            if interest_lower in domaine:
                score += 20
                matching_points.append(f"Int√©r√™t '{interest}' dans le domaine")

        if score > 0:
            results.append(
                {
                    "id": sujet.get("id", 0),
                    "score": min(score, 100),
                    "raisons": matching_points[:3] if matching_points else ["Correspondance g√©n√©rale"],
                    "crit√®res": [
                        "Matching automatique par mots-cl√©s",
                        f"Niveau: {sujet.get('niveau', 'N/A')}",
                        f"Domaine: {sujet.get('domaine', 'N/A')}",
                    ],
                }
            )

    results.sort(key=lambda x: x["score"], reverse=True)
    return results

def recommander_sujets_llm(
    interests: List[str],
    sujets: List[Dict],
    crit√®res: Dict[str, Any],
) -> List[Dict[str, Any]]:
    """Recommande des sujets avec LangChain"""
    if not llm or not sujets:
        return fallback_recommendation(interests, sujets)

    sujets_text = ""
    for sujet in sujets[:10]:
        sujets_text += f"\n‚Ä¢ ID: {sujet.get('id', 'N/A')}"
        sujets_text += f" | Titre: {sujet.get('titre', 'Sans titre')}"
        sujets_text += f" | Mots-cl√©s: {sujet.get('keywords', '')}"
        sujets_text += f" | Niveau: {sujet.get('niveau', 'N/A')}"
        sujets_text += f" | Domaine: {sujet.get('domaine', 'G√©n√©ral')}"

    prompt_template = """
    Tu es un assistant sp√©cialis√© dans la recommandation de sujets de m√©moire.

    **PROFIL √âTUDIANT:**
    - Int√©r√™ts: {interests}
    - Niveau: {niveau}
    - Facult√©: {facult√©}
    - Domaine: {domaine}
    - Difficult√©: {difficult√©}

    **SUJETS DISPONIBLES:**
    {sujets_text}

    **T√ÇCHE:**
    Pour les sujets les plus pertinents, fournis:
    1. Score de pertinence (0-100) bas√© sur les int√©r√™ts et crit√®res
    2. 2-3 raisons principales de recommandation
    3. Crit√®res d'acceptation respect√©s

    **FORMAT DE R√âPONSE (JSON):**
    [
{{
        "id": 1,
        "score": 85,
        "raisons": ["Raison 1", "Raison 2"],
        "crit√®res": ["Crit√®re 1", "Crit√®re 2"]
}}
    ]

    Retourne seulement les 3-5 sujets les plus pertinents, tri√©s par score d√©croissant.
    """

    try:
        prompt = ChatPromptTemplate.from_template(prompt_template)
        chain = prompt | llm | StrOutputParser()

        response = chain.invoke(
            {
                "interests": ", ".join(interests) if interests else "Non sp√©cifi√©",
                "niveau": crit√®res.get("niveau", "Non sp√©cifi√©"),
                "facult√©": crit√®res.get("facult√©", "Non sp√©cifi√©e"),
                "domaine": crit√®res.get("domaine", "Non sp√©cifi√©"),
                "difficult√©": crit√®res.get("difficult√©", "moyenne"),
                "sujets_text": sujets_text,
            }
        )

        try:
            json_match = re.search(r"\[.*\]", response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
                return result
        except (json.JSONDecodeError, AttributeError) as e:
            print(f"‚ö†Ô∏è Erreur parsing JSON recommandation: {e}")

        return fallback_recommendation(interests, sujets)

    except Exception as e:
        print(f"‚ö†Ô∏è Erreur recommandation LangChain: {e}")
        return fallback_recommendation(interests, sujets)

# ======================
# R√âPONSE √Ä UNE QUESTION
# ======================
def r√©pondre_question(question: str, contexte: str = None) -> str:
    """R√©pond DIRECTEMENT aux questions - version SIMPLIFI√âE et DIRECTE"""
    if not llm:
        return f"D'accord, je comprends ta question : '{question}'. Pourrais-tu me dire plus pr√©cis√©ment ce que tu recherches ?"
    
    # PROMPT ULTRA SIMPLE - PAS DE FORMALIT√âS
    prompt = f"""
    Tu es MemoBot, assistant conversationnel pour aider les √©tudiants √† trouver des sujets de m√©moire.
    
    **T√ÇCHE :** R√©ponds DIRECTEMENT et NATURELLEMENT √† la question de l'√©tudiant.
    **STYLE :** Comme si tu parlais √† un ami - simple, direct, utile.
    **NE FAIS PAS :** Ne commence pas par "Bonjour, je suis MemoBot..."
    **NE FAIS PAS :** Ne liste pas des questions en retour automatiquement
    
    CONTEXTE (si utile) :
    {contexte or 'Pas de contexte'}
    
    QUESTION DE L'√âTUDIANT :
    "{question}"
    
    TA R√âPONSE (directe, naturelle, utile) :
    """
    
    try:
        # Appel DIRECT sans LangChain complexe
        response = llm.invoke(prompt)
        
        # Extraire le texte
        if hasattr(response, 'content'):
            answer = response.content.strip()
        else:
            answer = str(response).strip()
        
        # NETTOYAGE : Enlever les salutations automatiques
        unwanted_starts = [
            "Bonjour ! Je suis MemoBot",
            "Je suis MemoBot",
            "En tant que MemoBot",
            "Bonjour,",
            "Salut,",
            "Hello,",
        ]
        
        for unwanted in unwanted_starts:
            if answer.startswith(unwanted):
                # Garder seulement apr√®s la salutation
                answer = answer[len(unwanted):].strip()
                # Supprimer la ponctuation qui suit
                if answer.startswith(','):
                    answer = answer[1:].strip()
                if answer.startswith('!'):
                    answer = answer[1:].strip()
        
        # Si la r√©ponse est vide ou trop courte, r√©ponse alternative
        if not answer or len(answer) < 10:
            return f"D'accord, je comprends que tu cherches : '{question}'. Qu'est-ce qui t'int√©resse particuli√®rement dans ce domaine ?"
        
        return answer
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur dans r√©pondre_question: {e}")
        return f"Je vois que tu parles de '{question[:50]}...'. C'est int√©ressant ! Dis-m'en plus sur ce que tu recherches exactement."

def r√©pondre_question_coh√©rente(question: str, contexte: str = None) -> str:
    """Version qui FORCE la coh√©rence avec l'historique"""
    if not llm:
        return f"Je comprends : '{question}'. Pourrais-tu pr√©ciser par rapport √† notre discussion ?"
    
    # Analyse le contexte pour d√©tecter le sujet en cours
    sujet_en_cours = None
    if contexte:
        contexte_lower = contexte.lower()
        if "g√©nie civil" in contexte_lower or "civil" in question.lower():
            sujet_en_cours = "g√©nie civil"
        elif "s√©curit√©" in contexte_lower or "s√©curit√©" in question.lower():
            sujet_en_cours = "s√©curit√©"
        elif "b√¢timent" in contexte_lower or "b√¢timent" in question.lower():
            sujet_en_cours = "b√¢timent"
    
    prompt = f"""
    Tu es MemoBot, assistant sp√©cialis√© dans les sujets de m√©moire acad√©miques.
    
    **CONTEXTE DE LA CONVERSATION:**
    {contexte or 'D√©but de conversation'}
    
    **SUJET EN COURS D√âTECT√â:** {sujet_en_cours or 'Non sp√©cifi√©'}
    
    **NOUVELLE QUESTION DE L'√âTUDIANT:**
    "{question}"
    
    **R√àGLES IMP√âRATIVES:**
    1. Reste ABSOLUMENT COH√âRENT avec l'historique
    2. Si le sujet change brusquement, dit: "Pour rester sur [sujet pr√©c√©dent]..."
    3. Ne parle pas d'autres sujets que celui en cours
    4. Sois utile pour la recherche d'un sujet de m√©moire
    5. Propose des pistes acad√©miques concr√®tes
    
    **TA R√âPONSE (coh√©rente, acad√©mique, utile):**
    """
    
    try:
        response = llm.invoke(prompt)
        answer = response.content if hasattr(response, 'content') else str(response)
        
        # V√©rification FORC√âE de coh√©rence
        if sujet_en_cours and sujet_en_cours not in answer.lower():
            # R√©ponse n'est pas coh√©rente, on force
            correction = f"""
            L'√©tudiant dit: "{question}"
            
            Mais nous parlions de: {sujet_en_cours}
            
            R√©ponds EN RESTANT sur le sujet {sujet_en_cours}.
            Commence par: "Pour rester sur le {sujet_en_cours}..."
            
            R√©ponse coh√©rente:
            """
            corrected = llm.invoke(correction)
            answer = corrected.content if hasattr(corrected, 'content') else str(corrected)
        
        return answer.strip()
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur dans r√©pondre_question_coh√©rente: {e}")
        if sujet_en_cours:
            return f"Pour rester sur le sujet du {sujet_en_cours}, {question[:50]}... Quel aspect pr√©cis veux-tu explorer ?"
        return f"Je comprends: '{question[:50]}...'. Quel lien fais-tu avec notre discussion pr√©c√©dente ?"

# ======================
# G√âN√âRATION DE SUJETS
# ======================

def generate_default_subjects(params: Dict[str, Any], count: int) -> List[Dict[str, Any]]:
    """G√©n√®re des sujets par d√©faut sans IA"""
    domaine = params.get("domaine", "Informatique")
    niveau = params.get("niveau", "Master")
    facult√© = params.get("facult√©", "Sciences")

    subjects = []
    for _ in range(count):
        subjects.append(
            {
                "titre": f"Application de l'IA dans le domaine du {domaine}",
                "probl√©matique": (
                    f"Comment l'intelligence artificielle peut-elle transformer les pratiques et "
                    f"processus dans le {domaine} ?"
                ),
                "keywords": f"IA, {domaine}, transformation, innovation, technologie",
                "description": (
                    f"√âtude des applications potentielles de l'intelligence artificielle dans le secteur du {domaine}, "
                    "avec une analyse des impacts et des d√©fis √† relever."
                ),
                "methodologie": "Revue de litt√©rature, analyse comparative, √©tude de cas",
                "difficult√©": "moyenne",
                "dur√©e_estim√©e": "6 mois",
                "domaine": domaine,
                "niveau": niveau,
                "facult√©": facult√©,
                "original": True,
                "generated_at": datetime.utcnow().isoformat(),
            }
        )

    return subjects

def g√©n√©rer_sujets_llm(params: Dict[str, Any], count: int) -> List[Dict[str, Any]]:
    """G√©n√®re des sujets avec LangChain"""
    if not llm:
        return generate_default_subjects(params, count)

    prompt_template = """
    Tu es un g√©n√©rateur de sujets de m√©moire universitaires.

    **SP√âCIFICATIONS:**
    - Int√©r√™ts: {interests}
    - Domaine: {domaine}
    - Niveau: {niveau}
    - Facult√©: {facult√©}
    - Nombre de sujets: {count}

    **EXIGENCES POUR CHAQUE SUJET:**
    1. Un titre pr√©cis et accrocheur
    2. Une probl√©matique claire et pertinente
    3. 5-7 mots-cl√©s s√©par√©s par des virgules
    4. Une description concise (2-3 phrases)
    5. Une m√©thodologie sugg√©r√©e
    6. Une difficult√© (facile/moyenne/difficile)
    7. Une dur√©e estim√©e (ex: 3-6 mois)

    **FORMAT DE R√âPONSE (JSON):**
    [
      {{
        "titre": "Titre du sujet",
        "probl√©matique": "Probl√©matique de recherche",
        "keywords": "mot1, mot2, mot3, mot4, mot5",
        "description": "Description du sujet",
        "methodologie": "M√©thodologie sugg√©r√©e",
        "difficult√©": "moyenne",
        "dur√©e_estim√©e": "6 mois"
      }}
    ]

    G√©n√®re exactement {count} sujets originaux, pertinents et r√©alisables.
    """

    try:
        prompt = ChatPromptTemplate.from_template(prompt_template)
        chain = prompt | llm | StrOutputParser()

        response = chain.invoke(
            {
                "interests": params.get("interests", "Recherche acad√©mique"),
                "domaine": params.get("domaine", "G√©n√©ral"),
                "niveau": params.get("niveau", "L3"),
                "facult√©": params.get("facult√©", "Sciences"),
                "count": count,
            }
        )

        try:
            json_match = re.search(r"\[.*\]", response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                sujets = json.loads(json_str)

                for sujet in sujets:
                    sujet["domaine"] = params.get("domaine", "G√©n√©ral")
                    sujet["niveau"] = params.get("niveau", "L3")
                    sujet["facult√©"] = params.get("facult√©", "Sciences")
                    sujet["original"] = True
                    sujet["generated_at"] = datetime.utcnow().isoformat()

                return sujets[:count]
        except (json.JSONDecodeError, AttributeError) as e:
            print(f"‚ö†Ô∏è Erreur parsing JSON g√©n√©ration: {e}")

        return generate_default_subjects(params, count)

    except Exception as e:
        print(f"‚ö†Ô∏è Erreur g√©n√©ration LangChain: {e}")
        return generate_default_subjects(params, count)

# ======================
# CONSEILS G√âN√âRAUX
# ======================

def get_tips() -> Dict[str, List[str]]:
    """
    Retourne des conseils pour la r√©daction de m√©moire
    """
    return {
        "choix_sujet": [
            "Choisissez un sujet qui vous passionne vraiment",
            "Assurez-vous que le sujet soit ni trop large ni trop √©troit",
            "V√©rifiez la disponibilit√© des ressources",
            "Le sujet doit apporter une contribution originale",
            "Consultez votre directeur potentiel avant de finaliser",
        ],
        "methodologie": [
            "D√©finissez clairement votre probl√©matique de recherche",
            "Choisissez une m√©thodologie adapt√©e √† votre question",
            "√âlaborez un plan de recherche d√©taill√©",
            "Documentez rigoureusement toutes vos sources",
            "Testez votre m√©thodologie sur un √©chantillon r√©duit",
        ],
        "redaction": [
            "Structurez votre m√©moire de mani√®re logique",
            "R√©digez r√©guli√®rement (un peu chaque jour)",
            "Utilisez un style acad√©mique clair et pr√©cis",
            "Citez vos sources selon les normes",
            "Faites relire votre travail par d'autres",
        ],
        "soutenance": [
            "Pr√©parez votre pr√©sentation bien √† l'avance",
            "Structurez votre pr√©sentation clairement",
            "Entra√Ænez-vous plusieurs fois √† pr√©senter",
            "Pr√©parez un support visuel professionnel",
            "Anticipez les questions du jury",
        ],
    }

# ======================
# TEST LOCAL
# ======================

if __name__ == "__main__":
    print("üß™ Test de LangChain avec Gemini...")

    if llm:
        try:
            prompt = ChatPromptTemplate.from_template("R√©ponds simplement 'OK' si tu fonctionnes.")
            chain = prompt | llm | StrOutputParser()
            response = chain.invoke({})
            print(f"‚úÖ LangChain fonctionne: {response}")
        except Exception as e:
            print(f"‚ùå Erreur test LangChain: {e}")
    else:
        print("‚ö†Ô∏è LangChain non configur√©, mode fallback activ√©")

    print("\n‚úÖ Module llm_service pr√™t")